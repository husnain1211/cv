{"cells":[{"cell_type":"markdown","metadata":{"id":"NMauZMalbLl7"},"source":["<img src=\"https://raw.githubusercontent.com/maxsitt/insect-detect-docs/main/docs/assets/logo.png\" width=\"500\">\n","\n","# YOLOv5 detection model training + conversion for deployment on Luxonis OAK\n","\n","> Compiled by: &nbsp; **Maximilian Sittinger** &nbsp;\n","[<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"24\">](https://github.com/maxsitt) &nbsp;\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/ORCID_iD.svg\" width=\"24\">](https://orcid.org/0000-0002-4096-8556)  \n","\n","- [`insect-detect-ml` GitHub repo](https://github.com/maxsitt/insect-detect-ml)\n","- ðŸ“‘ [**Insect Detect Docs**](https://maxsitt.github.io/insect-detect-docs/)\n","\n","&nbsp;\n","\n","**This notebook will enable you to train a [YOLOv5](https://github.com/ultralytics/yolov5) object detection model on your own custom training data.**\n","\n","- Using dataset import from [Roboflow](https://roboflow.com/) is recommended, but is not required.\n","> Choose option *Upload dataset from Google Drive/local file system* instead (slower!).  \n","- Connecting to Google Drive is recommended, but is not required.\n","> Choose options *Upload dataset from your local file system* and *Download* instead of *Export to Google Drive* (slower!).  \n","- If you are using Firefox, please make sure to allow notifications for this website.\n","\n","&nbsp;\n","\n","---\n","\n","**References**\n","\n","1. Official YOLOv5 tutorial notebook by Ultralytics &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)\n","1. Roboflow tutorial notebook for YOLOv5 training &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb)\n","1. DepthAI tutorial notebook + conversion to .blob &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luxonis/depthai-ml-training/blob/master/colab-notebooks/YoloV5_training.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3q8IjnB8QGG3"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"id":"pbdTb-q4QJ0M"},"source":["## Show GPU + Linux distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJ9oBX1dUhh"},"outputs":[],"source":["!nvidia-smi -L\n","print(\"\\n\")\n","!head -n 2 /etc/*release"]},{"cell_type":"markdown","metadata":{"id":"wVi4j857p0GS"},"source":["## YOLOv5 setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1F_lwcyqItL"},"outputs":[],"source":["!git clone https://github.com/ultralytics/yolov5\n","%cd yolov5\n","%pip install -qr requirements.txt\n","\n","import torch\n","import utils\n","display = utils.notebook_init()"]},{"cell_type":"markdown","metadata":{"id":"g41kdogczKts"},"source":["## Recommended: Upload dataset from Roboflow\n","\n","If you are not sure how to export your annotated dataset in YOLOv5 format, check the [Roboflow docs](https://docs.roboflow.com/exporting-data).\n","\n","> Alternatively you can upload your dataset ([YOLOv5 format](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#1-create-dataset)) from **[Google Drive](#scrollTo=RxOnnOadc5vR)** or directly from your **[local file system](#scrollTo=qKTCWdtkOUw7)** in the next steps!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRSmjO9MQRVq"},"outputs":[],"source":["%pip install -q roboflow\n","from roboflow import Roboflow\n","rf = Roboflow(model_format = \"yolov5\", notebook = \"insdet_yolov5\")"]},{"cell_type":"markdown","metadata":{"id":"8cqOYoopQx-U"},"source":["**Copy only the last three lines of the Download Code and insert them at the top of the next code cell:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SsPwQDzRvwH"},"outputs":[],"source":["%cd /content/yolov5\n","\n","### Paste your Download Code here:\n","rf = Roboflow(api_key=\"XXXXXXXXXXXXXXXXXXXXX\")\n","project = rf.workspace(\"maximilian-sittinger\").project(\"insect_detect_detection\")\n","dataset = project.version(4).download(\"yolov5\")\n","###\n","\n","dataset_location = dataset.location\n","\n","from pathlib import Path\n","print(\"\\n\")\n","print(f\"Location of dataset: {dataset_location}\")\n","print(\"\\n\")\n","print(\"Number of training images:\", len(list(Path(f\"{dataset_location}/train/images\").glob(\"*.jpg\"))))\n","print(\"Number of validation images:\", len(list(Path(f\"{dataset_location}/valid/images\").glob(\"*.jpg\"))))\n","print(\"Number of test images:\", len(list(Path(f\"{dataset_location}/test/images\").glob(\"*.jpg\"))))\n","print(\"\\n\")\n","%cat {dataset_location}/data.yaml"]},{"cell_type":"markdown","metadata":{"id":"RxOnnOadc5vR"},"source":["## Recommended: Connect to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4lMoPNddCtx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFA-ROJ8rUWU"},"outputs":[],"source":["#@title ## Optional: Upload dataset from Google Drive {display-mode: \"form\"}\n","\n","#@markdown ### Google Drive path to dataset folder:\n","dataset_path = \"MyDrive/Datasets/yolov5_dataset\" #@param {type: \"string\"}\n","\n","%cp -ai /content/drive/{dataset_path} /content/yolov5\n","\n","from pathlib import Path\n","dataset_name = Path(dataset_path).stem\n","dataset_location = f\"/content/yolov5/{dataset_name}\"\n","\n","print(f\"Location of dataset: {dataset_location}\")\n","print(\"\\n\")\n","print(\"Number of training images:\", len(list(Path(f\"{dataset_location}/train/images\").glob(\"*.jpg\"))))\n","print(\"Number of validation images:\", len(list(Path(f\"{dataset_location}/valid/images\").glob(\"*.jpg\"))))\n","print(\"Number of test images:\", len(list(Path(f\"{dataset_location}/test/images\").glob(\"*.jpg\"))))\n","print(\"\\n\")\n","%cat {dataset_location}/data.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKTCWdtkOUw7"},"outputs":[],"source":["#@title ## Optional: Upload dataset from your local file system {display-mode: \"form\"}\n","\n","#@markdown ### Name of your (zipped) dataset folder:\n","#@markdown Please make sure to compress your dataset folder to zip file before uploading!\n","dataset_name = \"yolov5_dataset\" #@param {type: \"string\"}\n","dataset_location = f\"/content/yolov5/{dataset_name}\"\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","!unzip -uq {dataset_name}.zip -d /content/yolov5/\n","%rm {dataset_name}.zip\n","\n","from pathlib import Path\n","print(\"\\n\")\n","print(f\"Location of dataset: {dataset_location}\")\n","print(\"\\n\")\n","print(\"Number of training images:\", len(list(Path(f\"{dataset_location}/train/images\").glob(\"*.jpg\"))))\n","print(\"Number of validation images:\", len(list(Path(f\"{dataset_location}/valid/images\").glob(\"*.jpg\"))))\n","print(\"Number of test images:\", len(list(Path(f\"{dataset_location}/test/images\").glob(\"*.jpg\"))))\n","print(\"\\n\")\n","%cat {dataset_location}/data.yaml"]},{"cell_type":"markdown","metadata":{"id":"uv36pkEtMupF"},"source":["## Optional: Edit *data.yaml*\n","If you chose to upload your training dataset from **Google Drive** or your **local file system**, you have to edit the `data.yaml` file in your dataset folder to make sure the paths to the train, valid and test folders are correct.\n","\n","Navigate to */content/yolov5* in the File Explorer (Folder symbol on the left side bar) and open your dataset folder. Double-click on the `data.yaml` file, it will open in the editor to the right. Make sure that the paths to the train, valid and test folders are as follows:\n","\n","```\n","train: /content/yolov5/[YOUR_DATASET_NAME]/train/images\n","val: /content/yolov5/[YOUR_DATASET_NAME]/valid/images\n","test: /content/yolov5/[YOUR_DATASET_NAME]/test/images\n","```\n","\n","* Insert the correct name of your dataset folder at `[YOUR_DATASET_NAME]`.\n","* Save your changes with **Ctrl + S** and close the editor."]},{"cell_type":"markdown","metadata":{"id":"nnn4pSbI6eTv"},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IxTKp7CjuOA"},"outputs":[],"source":["#@title ## Optional: Select external logger {display-mode: \"form\"}\n","\n","logger = \"Weights&Biases\" #@param [\"Weights&Biases\", \"Comet\", \"ClearML\"]\n","\n","#@markdown **More info:** \\\n","#@markdown - [Weights & Biases](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/wandb)\n","#@markdown - [Comet](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet)\n","#@markdown - [ClearML](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml)\n","\n","if logger == \"Weights&Biases\":\n","  %pip install -q wandb\n","  import wandb\n","  wandb.login()\n","elif logger == \"Comet\":\n","  %pip install -q comet_ml\n","  import comet_ml\n","  comet_ml.init()\n","elif logger == \"ClearML\":\n","  %pip install -q clearml\n","  import clearml\n","  clearml.browser_login()"]},{"cell_type":"markdown","metadata":{"id":"U4t4JhGYGOcr"},"source":["## Tensorboard logger\n","\n","> If you are using Firefox, **disable Enhanced Tracking Protection** for this website (click on the shield to the left of the address bar) for the Tensorboard logger to work correctly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYCUyGITGU6j"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/yolov5/runs/train"]},{"cell_type":"markdown","metadata":{"id":"7t6PUcz3SsEy"},"source":["## Train YOLOv5 detection model\n","\n","- `--name` name of the training run\n","- `--img` input image size (recommended: same size as for inference)\n","- `--batch` determine [batch size](https://github.com/ultralytics/yolov5/issues/2377) (recommended: 32)\n","- `--epochs` set the number of training [epochs](https://machine-learning.paperspace.com/wiki/epoch) (recommended: 100-300+ epochs)\n","- `--data` path to YAML file\n","- `--weights` specify the pretrained model weights\n","> `--weights yolov5n.pt` for YOLOv5-nano model  \n","`--weights yolov5s.pt` for YOLOv5-small model (recommended)  \n","`--weights yolov5m.pt` for YOLOv5-medium model\n","\n","- `--cache:` cache images for faster training\n","\n","> More information on [model training](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results) ðŸš€"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAYNJg9M7sg3"},"outputs":[],"source":["training_run_name = \"YOLOv5s_1class_416_batch32_epochs250\" #@param {type: \"string\"}\n","#@markdown **Add UTC timestamp in front of training run name:**\n","add_timestamp = True #@param {type:\"boolean\"}\n","#@markdown ---\n","image_size = 416 #@param {type: \"integer\"}\n","batch_size = 32 #@param {type:\"slider\", min:16, max:128, step:16}\n","number_epochs = 250 #@param {type:\"slider\", min:10, max:600, step:10}\n","weights = \"yolov5s.pt\" #@param [\"yolov5n.pt\", \"yolov5s.pt\", \"yolov5m.pt\"]\n","\n","if add_timestamp == True:\n","  from datetime import datetime\n","  utc_timestamp = datetime.now().strftime(\"%Y%m%d_%H-%M\")\n","  train_run_name = f\"{utc_timestamp}_{training_run_name}\"\n","else:\n","  train_run_name = training_run_name\n","\n","%cd /content/yolov5\n","\n","!python train.py \\\n","--name {train_run_name} \\\n","--img {image_size} \\\n","--batch {batch_size} \\\n","--epochs {number_epochs} \\\n","--data {dataset_location}/data.yaml \\\n","--weights {weights} \\\n","--cache"]},{"cell_type":"markdown","metadata":{"id":"dQ0Q2_YqE7Px"},"source":["### View metrics plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fHyUtSBhE8hf"},"outputs":[],"source":["from IPython.display import Image\n","Image(filename = f\"/content/yolov5/runs/train/{train_run_name}/results.png\", width=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h90_4rFQx0mp"},"outputs":[],"source":["#@title ## Export to Google Drive or Download training results {display-mode: \"form\"}\n","\n","training_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving training results in Google Drive:\n","GDrive_save_path = \"MyDrive/Training_results/YOLOv5\"  #@param {type: \"string\"}\n","\n","if training_results == \"Export_Google_Drive\":\n","  %mkdir -p /content/drive/{GDrive_save_path}\n","  %cp -ai /content/yolov5/runs/train/{train_run_name} /content/drive/{GDrive_save_path}\n","elif training_results == \"Download\":\n","  %cd /content/yolov5/runs/train\n","  !zip -rq {train_run_name}.zip {train_run_name}\n","  from google.colab import files\n","  files.download(f\"{train_run_name}.zip\")"]},{"cell_type":"markdown","metadata":{"id":"k54rL7jSM_ni"},"source":["## Validate on dataset test split\n","\n","If you want to validate on the dataset valid split (default), remove the line `--task \"test\" \\`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPKGJ1k8NDmo"},"outputs":[],"source":["%cd /content/yolov5\n","\n","!python val.py \\\n","--name {train_run_name}_validate \\\n","--weights runs/train/{train_run_name}/weights/best.pt \\\n","--data {dataset_location}/data.yaml \\\n","--img {image_size} \\\n","--task \"test\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LrEr4uSVG6p"},"outputs":[],"source":["#@title ## Export to Google Drive or Download validation results {display-mode: \"form\"}\n","\n","validation_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving validation results in Google Drive:\n","GDrive_save_path = \"MyDrive/Training_results/YOLOv5\"  #@param {type: \"string\"}\n","\n","if validation_results == \"Export_Google_Drive\":\n","  %mkdir -p /content/drive/{GDrive_save_path}\n","  %cp -ai /content/yolov5/runs/val/{train_run_name}_validate /content/drive/{GDrive_save_path}\n","elif validation_results == \"Download\":\n","  %cd /content/yolov5/runs/val\n","  !zip -rq {train_run_name}_validate.zip {train_run_name}_validate\n","  from google.colab import files\n","  files.download(f\"{train_run_name}_validate.zip\")"]},{"cell_type":"markdown","metadata":{"id":"Rs7bZGuQROhk"},"source":["## Test inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T99Qs1noRTbb"},"outputs":[],"source":["#@markdown #### Decrease confidence threshold to detect objects with lower confidence score\n","confidence_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","\n","%cd /content/yolov5\n","\n","!python detect.py \\\n","--name {train_run_name}_detect \\\n","--weights runs/train/{train_run_name}/weights/best.pt \\\n","--source {dataset_location}/test/images/ \\\n","--img {image_size} \\\n","--conf-thres {confidence_threshold} \\\n","--line-thickness 1  # bounding box line thickness and label size (default: 3)\n","#--visualize        # enable feature map visualization"]},{"cell_type":"markdown","metadata":{"id":"3_XwmhjIXnFt"},"source":["### Show inference results on test images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsskeC4jRrj8"},"outputs":[],"source":["import glob\n","from IPython.display import Image, display\n","\n","for imageName in glob.glob(f\"/content/yolov5/runs/detect/{train_run_name}_detect/*.jpg\"):\n","  display(Image(filename=imageName))\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"KiU6Qt79ShXJ"},"source":["# Model conversion\n","\n","You can upload your model weights file (**best.pt**) directly at https://tools.luxonis.com/ to automatically convert your model to .blob file.\n","\n","**For manual conversion (more options) continue with the following steps.**"]},{"cell_type":"markdown","metadata":{"id":"4kI7M2lLZDrt"},"source":["## Export trained model weights to ONNX format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5z0SWsvVH_8"},"outputs":[],"source":["%cd /content/yolov5\n","\n","!python export.py \\\n","--weights runs/train/{train_run_name}/weights/best.pt \\\n","--include onnx \\\n","--simplify"]},{"cell_type":"markdown","metadata":{"id":"lF4oZlkxQtAK"},"source":["### Edit outputs of the YOLOv5 ONNX model\n","\n","We have to slightly edit the outputs of the YOLOv5 model for it to work with the [YoloDetectionNetwork](https://docs.luxonis.com/projects/api/en/latest/components/nodes/yolo_detection_network/) node in the DepthAI API, which was initially developed for YOLOv3 and YOLOv4.\n","\n","We will cut off the last layers for post-processing, as these processing steps will be done on the OAK device during inference. The actual cutting is done in the next step when converting the model to the OpenVINO IR format, first we will define the new output layers of the ONNX model.\n","\n","> More information on model cutting can be found at the [OpenVINO docs](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Cutting_Model.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7K_0SrrRoRC"},"outputs":[],"source":["%pip install -q onnx\n","import onnx\n","\n","onnx_model = onnx.load(f\"/content/yolov5/runs/train/{train_run_name}/weights/best.onnx\")\n","\n","conv_indices = []\n","for i, n in enumerate(onnx_model.graph.node):\n","  if \"Conv\" in n.name:\n","    conv_indices.append(i)\n","input1, input2, input3 = conv_indices[-3:]\n","sigmoid1 = onnx.helper.make_node(\n","  'Sigmoid',\n","  inputs=[onnx_model.graph.node[input1].output[0]],\n","  outputs=['output1_yolov5'],\n",")\n","sigmoid2 = onnx.helper.make_node(\n","  'Sigmoid',\n","  inputs=[onnx_model.graph.node[input2].output[0]],\n","  outputs=['output2_yolov5'],\n",")\n","sigmoid3 = onnx.helper.make_node(\n","  'Sigmoid',\n","  inputs=[onnx_model.graph.node[input3].output[0]],\n","  outputs=['output3_yolov5'],\n",")\n","onnx_model.graph.node.append(sigmoid1)\n","onnx_model.graph.node.append(sigmoid2)\n","onnx_model.graph.node.append(sigmoid3)\n","\n","onnx.save(onnx_model, f\"/content/yolov5/runs/train/{train_run_name}/weights/best_cut.onnx\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bnFoFi-dY_I"},"outputs":[],"source":["#@title ## Export to Google Drive or Download ONNX models {display-mode: \"form\"}\n","\n","onnx_models = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving ONNX models in Google Drive:\n","GDrive_save_path = \"MyDrive/Training_results/YOLOv5\"  #@param {type: \"string\"}\n","\n","%mkdir -p /content/yolov5/runs/train/{train_run_name}/weights/onnx\n","%cp -ai /content/yolov5/runs/train/{train_run_name}/weights/best.onnx /content/yolov5/runs/train/{train_run_name}/weights/onnx\n","%cp -ai /content/yolov5/runs/train/{train_run_name}/weights/best_cut.onnx /content/yolov5/runs/train/{train_run_name}/weights/onnx\n","\n","if onnx_models == \"Export_Google_Drive\":\n","  %mkdir -p /content/drive/{GDrive_save_path}\n","  %cp -ai /content/yolov5/runs/train/{train_run_name}/weights/onnx /content/drive/{GDrive_save_path}/{train_run_name}/weights\n","elif onnx_models == \"Download\":\n","  %cd /content/yolov5/runs/train\n","  !zip -rq {train_run_name}/weights/onnx.zip {train_run_name}/weights/onnx\n","  from google.colab import files\n","  files.download(f\"{train_run_name}/weights/onnx.zip\")"]},{"cell_type":"markdown","metadata":{"id":"weAYjPwhZY0O"},"source":["## Convert ONNX model to OpenVINO IR format\n","\n","> More information on model optimization can be found at the [OpenVINO docs](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ1ALQpCCBby"},"outputs":[],"source":["#@markdown ### Name for the OpenVINO IR model:\n","model_name = \"yolov5s_416_1class\"  #@param {type: \"string\"}\n","#@markdown ---\n","\n","image_size = 416 #@param {type: \"integer\"}\n","\n","%pip install -q openvino-dev==2022.1.0\n","\n","!mo \\\n","--input_model /content/yolov5/runs/train/{train_run_name}/weights/best_cut.onnx \\\n","--model_name {model_name} \\\n","--output_dir /content/yolov5/runs/train/{train_run_name}/weights/openvino/ \\\n","--input_shape [1,3,{image_size},{image_size}] \\\n","--scale 255 \\\n","--reverse_input_channels \\\n","--output \"output1_yolov5,output2_yolov5,output3_yolov5\" \\\n","--data_type FP16"]},{"cell_type":"markdown","metadata":{"id":"S09xOqQJnON6"},"source":["## Convert OpenVINO IR to .blob file\n","\n","> More information on blob conversion can be found at the [DepthAI docs](https://docs.luxonis.com/en/latest/pages/model_conversion/).\n","\n","> More information on the number of shaves can be found at the [DepthAI FAQ](https://docs.luxonis.com/en/latest/pages/faq/#what-are-the-shaves)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIOfNQPLnSUH"},"outputs":[],"source":["#@markdown ### Specify number of SHAVE cores that the converted model will use\n","#@markdown Recommended number of shaves: **4 - 9**\n","number_shaves = 9 #@param {type:\"slider\", min:1, max:16, step:1}\n","\n","%pip install -q blobconverter boto3==1.17.39\n","import blobconverter\n","\n","blob_path = blobconverter.from_openvino(\n","  xml = f\"/content/yolov5/runs/train/{train_run_name}/weights/openvino/{model_name}.xml\",\n","  bin = f\"/content/yolov5/runs/train/{train_run_name}/weights/openvino/{model_name}.bin\",\n","  data_type = \"FP16\",\n","  shaves = number_shaves,\n","  version = \"2022.1\",\n","  output_dir = f\"/content/yolov5/runs/train/{train_run_name}/weights/openvino/blob/\",\n","  use_cache = False\n",")"]},{"cell_type":"markdown","metadata":{"id":"CTig30QH_Fcy"},"source":["## Generate config JSON file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRHnlK9SUjbH"},"outputs":[],"source":["#@markdown ### Name for the config JSON file:\n","json_name = \"yolov5s_416_1class\" #@param {type: \"string\"}\n","#@markdown ---\n","\n","image_size = 416 #@param {type: \"integer\"}\n","number_classes = 1 #@param {type: \"integer\"}\n","#@markdown ---\n","\n","#@markdown #### For several classes/labels: **[\"class1\", \"class2\", \"class3\"]**\n","labels = [\"insect\"] #@param {type: \"raw\"}\n","#@markdown ---\n","\n","#@markdown #### Decrease confidence threshold to detect objects with lower confidence score:\n","confidence_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","#@markdown #### Increase IoU threshold if the same object is detected multiple times:\n","iou_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","\n","masks = {\n","  f\"side{int(image_size/8)}\" : [0,1,2],\n","  f\"side{int(image_size/16)}\" : [3,4,5],\n","  f\"side{int(image_size/32)}\" : [6,7,8]\n","}\n","\n","!wget -L https://raw.githubusercontent.com/luxonis/depthai-experiments/master/gen2-yolo/device-decoding/json/yolov5.json -P /content/\n","\n","import json\n","\n","with open(\"/content/yolov5.json\", \"r\") as f:\n","  json_data = json.load(f)\n","  json_data[\"nn_config\"][\"input_size\"] = f\"{image_size}x{image_size}\"\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"classes\"] = number_classes\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"anchor_masks\"] = masks\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"iou_threshold\"] = iou_threshold\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"confidence_threshold\"] = confidence_threshold\n","  json_data[\"mappings\"][\"labels\"] = labels\n","\n","with open(f\"/content/yolov5/runs/train/{train_run_name}/weights/openvino/blob/{json_name}.json\", \"w\") as f:\n","  json.dump(json_data, f, indent = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpEBi9-RtjR2"},"outputs":[],"source":["#@title ## Export to Google Drive or Download OpenVINO and blob model + config JSON {display-mode: \"form\"}\n","\n","openvino_models = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving OpenVINO models in Google Drive:\n","GDrive_save_path = \"MyDrive/Training_results/YOLOv5\"  #@param {type: \"string\"}\n","\n","if openvino_models == \"Export_Google_Drive\":\n","  %mkdir -p /content/drive/{GDrive_save_path}\n","  %cp -ai /content/yolov5/runs/train/{train_run_name}/weights/openvino /content/drive/{GDrive_save_path}/{train_run_name}/weights\n","elif openvino_models == \"Download\":\n","  %cd /content/yolov5/runs/train\n","  !zip -rq {train_run_name}/weights/openvino.zip {train_run_name}/weights/openvino\n","  from google.colab import files\n","  files.download(f\"{train_run_name}/weights/openvino.zip\")"]},{"cell_type":"markdown","metadata":{"id":"g_SAn96loGK-"},"source":["# Model deployment\n","\n","That's it! You trained your own YOLOv5 object detection model with your custom dataset and converted it to blob format which is necessary to run inference on the [Luxonis OAK devices](https://docs.luxonis.com/projects/hardware/en/latest/).\n","\n","> To deploy the YOLOv5 model on your OAK, you can check out the Luxonis GitHub repository for [on-device decoding](https://github.com/luxonis/depthai-experiments/tree/master/gen2-yolo/device-decoding) or use the deployment options from the [Insect Detect Docs](link) (e.g. for continuous automated insect monitoring)."]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"1CDz0HUpYTTLxmvPpCx5b2nVyjY_LIYQ6","timestamp":1671662125129}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"baf2788c67905bf5eabce425833f665485fde887eca8cd7474f373ca3e9af677"}}},"nbformat":4,"nbformat_minor":0}